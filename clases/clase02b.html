<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>clase02b.utf8</title>
    <meta charset="utf-8" />
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/xaringanExtra_fit-screen/fit-screen.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


background-image: url(img/portada-flacso.png)
background-size: cover
class: animated slideInRight fadeOutLeft, middle



























# Machine Learning en Aplicaciones Espaciales


### Clase 2b: Introducci칩n a ensembles. Random Forest 


---


## Ensembles


Con el nombre de ensembles se conocen a los modelos de:

.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt4[

* **Variedades de Bagging** (Bagged Trees, Random Forest, Extra Trees)

* **Variedades de Boosting** (Adaboost, XGBoost)


]


--

La idea es combinar modelos simples (치rboles de decisi칩n) para lograr **una mejor predicci칩n**.

--

Usados en regresi칩n y clasificaci칩n.

---

## Bias y Varianza en 치rboles de decisi칩n

* Por lo general, los 치rboles peque침os (pocas ramificaciones) tienen poca varianza pero no consiguen representar bien la relaci칩n entre las variables, es decir, tienen bias alto. En contraposici칩n, los 치rboles grandes se ajustan mucho a los datos de entrenamiento, por lo que tienen muy poco bias pero mucha varianza. Una forma de solucionar este problema son los m칠todos de ensemble.

* Los **m칠todos de ensemble** combinan m칰ltiples modelos en uno nuevo con el objetivo de **lograr un equilibro entre bias y varianza**, consiguiendo as칤 mejores predicciones que cualquiera de los modelos individuales originales.

---

### 쯇or qu칠 bagging?


&lt;img src="img/bagging-trees.png" width="90%" style="display: block; margin: auto;" /&gt;

El boostrapping o bagging introduce mayor diversidad en los subsets de datos en los que cada predictor es entrenado, lo que trae como conseucencia que el bias sea m치s alto que si no se hiciese bagging. Esto significa que el predictor termina siendo menos correlacionado, entonces la varianza del ensemble se reduce. En general, el bagging resulta en mejores modelos, lo que explica porque se lo prefiere a DT. 


.footnote[Fuente: Hands on Machine Learning with scikit-learn and Tensorflow]

---

## Bias y varianza en bagging / boosting


.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt3[
###  En *bagging*, se emplean **modelos con muy poco bias pero mucha varianza**, agreg치ndolos se consigue reducir la varianza sin apenas aumentar el bias. 
###  En *boosting*, se emplean **modelos con muy poca varianza pero mucho bias**, ajustando secuencialmente los modelos se reduce el bias. Por lo tanto, cada una de las estrategias reduce una parte del error total.

]



---

## Bagging para clasificaci칩n

* Tambi칠n llamado *bootstrap resampling*. 


.left-column[
&lt;img src="img/bagging0.svg" width="400%" style="display: block; margin: auto;" /&gt;


]

.right-column[

&lt;img src="img/bagging.svg" width="100%" style="display: block; margin: auto;" /&gt;

]



---
## Bagging para clasificaci칩n

.left-column[
&lt;img src="img/bagging0.svg" width="100%" style="display: block; margin: auto;" /&gt;


]

.right-column[

&lt;img src="img/bagging_line.svg" width="200%" style="display: block; margin: auto;" /&gt;


&lt;img src="img/bagging_trees.svg" width="200%" style="display: block; margin: auto;" /&gt;


]


---

## Bagging para clasificaci칩n



.left-column[
&lt;img src="img/bagging0_cross.svg" width="100%" style="display: block; margin: auto;" /&gt;


]

.right-column[

&lt;img src="img/bagging_cross.svg" width="120%" style="display: block; margin: auto;" /&gt;


&lt;img src="img/bagging_trees_predict.svg" width="120%" style="display: block; margin: auto;" /&gt;


&lt;img src="img/bagging_vote.svg" width="120%" style="display: block; margin: auto;" /&gt;


]



---


## Baggging para regresi칩n



.left-column[
&lt;img src="img/bagging_reg_data.svg" width="100%" style="display: block; margin: auto;" /&gt;


]

.right-column[

&lt;img src="img/bagging_reg_grey.svg" width="120%" style="display: block; margin: auto;" /&gt;


&lt;img src="img/bagging_reg_grey_fitted.svg" width="120%" style="display: block; margin: auto;" /&gt;

Promediamos


&lt;img src="img/bagging_reg_blue.svg" width="40%" style="display: block; margin: auto;" /&gt;
]





---

## Algoritmo de Bagging



&lt;img src="img/Bagging-algo.png" width="100%" style="display: block; margin: auto;" /&gt;



---

## Random Forest

Los m칠todos de random forest y bagging siguen el mismo algoritmo con la 칰nica diferencia de que, en random forest, **antes de cada divisi칩n, se seleccionan aleatoriamente m predictores**. La diferencia en el resultado depender치 del valor m escogido. 

* Si **m=p**, los resultados de random forest y bagging son equivalentes. 

* Para *clasificaci칩n*, por defecto, el valor de m, es la ra칤z cuadrada del n칰mero total de predictores. **m = sqrt(p)**

* Para *regresi칩n*, el valor por defecto de m, es un tercio del n칰mero de predictores. **m = p/3**

Si los predictores est치n muy correlacionados, valores peque침os de m, consiguen mejores resultados. En la pr치ctica, tuneamos el valor de **m** con **mtry**. 

---

## 쮺칩mo sucede esto?

* Supongamosque hay un predictor muy bueno en el conjunto de datos, junto con otros varios predictores moderadamente buenos. Luego en la colecci칩n de *bagged trees*, la mayor칤a o todos los 치rboles utilizar치n este potente conjunto de predictores para realizar los splits. 

--
En consecuencia, todos los *bagged trees* se ver치n bastante similares entre s칤. **Por lo tanto, las predicciones de los 치rboles en bolsas estar치n altamente correlacionadas.** Desafortunadamente, promediar muchas cantidades altamente correlacionadas no conduce a una reducci칩n en la varianza tan grande como promediar muchas cantidades no correlacionadas.  En particular, esto significa que el bagging no dar치 lugar a una reducci칩n de la varianza sobre un solo 치rbol en este entorno.

--

* *Random Forest* superan este problema obligando a cada divisi칩n a considerar solo un subconjunto de los predictores. Por lo tanto, en promedio (p - m) / p de las divisiones ni siquiera considerar치n el predictor fuerte, por lo que otros predictores tendr치 m치s posibilidades. **Podemos pensar en este proceso como una decorrelaci칩n los 치rboles**, lo que hace que el promedio de los 치rboles resultantes sea menos variable
y por lo tanto m치s confiable.
La principal diferencia entre *bagged trees* y *random forest* es la elecci칩n del tama침o del subconjunto de predictores m.





---
## Retomando 

.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph3.mt4[
### Bagging es una estrategia general para cualquier tipo de modelo. 


### En Random Forest, en particular, aparte de bagging, hago un sampling de variables, que me permite reducir la varianza y obtener mejores predicciones. 
]



---

## Esquema de Random Forest




&lt;img src="img/random-forest.png" width="90%" style="display: block; margin: auto;" /&gt;


---

## Out-of-Bag error

* El hecho de que los 치rboles se ajusten empleando muestras generadas por bootstrapping conlleva que, en promedio, cada ajuste use solo aproximadamente dos tercios de las observaciones originales. Al tercio restante se le llama out-of-bag (OOB).


--

* Si para cada 치rbol ajustado en el proceso de bagging se registran las observaciones empleadas, se puede predecir la respuesta de la observaci칩n i haciendo uso de aquellos 치rboles en los que esa observaci칩n ha sido excluida y promedi치ndolos (la moda en el caso de los 치rboles de clasificaci칩n). Siguiendo este proceso, se pueden obtener las predicciones para las n observaciones y con ellas calcular el **OOB-mean square error (para regresi칩n)** o el **OOB-classification error (para 치rboles de clasificaci칩n)**. 


---

## Limitaciones del out-of-bag error


.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[

* No adecuado para datos que guardan una relaci칩n temporal (series temporales).

* El preprocesado de los datos de entrenamiento se hace de forma conjunta, por lo que las observaciones out-of-bag pueden sufrir data leakage. De ser as칤, las estimaciones del OOB-error son demasiado optimistas.

]




---
## Random Forest



.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[
### Random Forest es un algoritmo sencillo, f치cil de implementar, f치cil de usar y requiere tunning de pocos hiperpar치metros. 
### A diferencia de 츼rboles de Decisi칩n puede ser menos interpretable. 

]


---

class: inverse, center, middle

## Si tenemos muchos 치rboles como sucede en random forest

--

## 쮺칩mo s칠 que variables son m치s importantes?


---

## Importancia de las variables 



---

## Algoritmo de Random Forest



&lt;img src="img/rf-algo.png" width="100%" style="display: block; margin: auto;" /&gt;






---


## Tunning en Random Forest en R



&lt;img src="img/hiper-rf.png" width="90%" style="display: block; margin: auto;" /&gt;




.footnote[Fuente: https://parsnip.tidymodels.org/reference/rand_forest.html]


---

## Extra Trees

* Extra Trees = Extremely Randomized Trees

Cuando crece un 치rbol en un Random Forest, en cada nodo solo una parte aleatoria de los features es considerada para el split. Es posible hacer los 치rboles a칰n m치s aleatorios usando umbrales aleatorios para cada feature (variable) en vez de buscar los mejores posibles umbrales (como se hace en 치rboles de decisi칩n). 

Un 치rbol de 치rboles tan aleatorios, simplemente se los llama **Extremely Randomized trees** o ExtraTrees. De vuelta, esto intercambia m치s bias por menos varianza. Esto hace que los ExtraTrees **m치s r치pidos de entrenar que Random Forests** ya que encontrar el mejor umbral posible para cada feature a cada nodo es la tarea m치s cara computacionalmente. 



.footnote[Fuente: Hands on Machine learning with scikit-learn and Tensorflow]



---

## Extra Trees




&lt;img src="img/extra-T-comp.png" width="100%" style="display: block; margin: auto;" /&gt;




.footnote[Fuente: https://www.youtube.com/watch?v=r5C3TUIw6Zk&amp;t=1465s]


---

class: inverse, center, middle

## Manos en R! 游뗿


---

## Bibliograf칤a 


* Introduction to Statistical Learning. Cap칤tulo 8. 

* The Elements of Statistical Learning. Cap칤tulo 15.

* Hands on Machine learning with scikit-learn and Tensorflow. Cap칤tulo 7.

* Applied Predictive Modeling. Cap칤tulo 8.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
