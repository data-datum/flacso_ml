<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>clase01b.utf8</title>
    <meta charset="utf-8" />
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/xaringanExtra_fit-screen/fit-screen.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


background-image: url(img/portada-flacso.png)
background-size: cover
class: animated slideInRight fadeOutLeft, middle



























# Machine Learning en Aplicaciones Espaciales


### Clase 1b: √Årboles de Decisi√≥n


---

## √Årboles de decisi√≥n.



* Los √°rboles de decisi√≥n son modelos predictivos formados por reglas binarias (si/no) con las que se consigue repartir las observaciones en funci√≥n de sus atributos y predecir as√≠ el valor de la variable respuesta.



--

* Los √°rboles de regresi√≥n son el subtipo de √°rboles de predicci√≥n que se aplica cuando la variable respuesta es continua. En t√©rminos generales, en el entrenamiento de un √°rbol de regresi√≥n, las observaciones se van distribuyendo por bifurcaciones (nodos) generando la estructura del √°rbol hasta alcanzar un nodo terminal. Cuando se quiere predecir una nueva observaci√≥n, se recorre el √°rbol acorde al valor de sus predictores hasta alcanzar uno de los nodos terminales. La predicci√≥n del √°rbol es la media de la variable respuesta de las observaciones de entrenamiento que est√°n en ese mismo nodo terminal.

--

* Pueden usarse para **regresi√≥n** y **clasificaci√≥n**




---

## Veamoslo en un ejemplo

El set de datos Hitter contiene informaci√≥n sobre 322 jugadores de b√©isbol de la liga profesional. Entre las variables registradas para cada jugador se encuentran: el salario (Salary), a√±os de experiencia (Years) y el n√∫mero de bateos durante los √∫ltimos a√±os (Hits). Utilizando estos datos, se quiere predecir el salario (en unidades logar√≠tmicas) de un jugador en base a su experiencia y n√∫mero de bateos.

--



&lt;img src="img/tree-11.png" width="50%" style="display: block; margin: auto;" /&gt;
---


Como resultado de las bifurcaciones, se han generado 3 regiones que pueden identificarse con la siguiente nomenclatura:

`\(R_{1} = \left \{ X | Year &lt; 4.5  \right \}\)` : jugadores que han jugado menos de 4.5 a√±os.

`\(R_{2} = \left \{ X | Year \geq   4.5, Hits &lt;  117.5 \right \}\)` : jugadores que han jugado 4.5 a√±os o m√°s y que han conseguido menos de 117.5 bateos.

`\(R_{3} = \left \{ X | Year \geq   4.5, Hits  \geq   117.5 \right \}\)` : jugadores que han jugado 4.5 a√±os o m√°s y que han conseguido 117.5 o m√°s bateos.



&lt;img src="img/decision-t1.png" width="60%" style="display: block; margin: auto;" /&gt;


---


class: inverse, center, middle



## ¬øPodemos seguir dividiendo el √°rbol?

--
## ¬øHasta cuando?

---

## ¬øPodemos seguir dividiendo el √°rbol?

.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[
En el caso de los √°rboles de regresi√≥n, el criterio empleado con m√°s frecuencia para identificar las divisiones es el **Residual Sum of Squares (RSS)**. El objetivo es encontrar las J regiones (R1,‚Ä¶, Rj) que minimizan el Residual Sum of Squares (RSS) total

`\(RSS = \sum_{j=1}^{J}\sum_{i \epsilon R_{j}}^{} (y_{i} - \widehat{y}_{R_{j}})^{2}\)`

]


--

## ¬øHasta cuando?


.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[
Si el √°rbol sigue creciendo se va a volver muy bueno para nuestros datos, es decir, va a disminuir el error de entrenamiento, pero no va a ser capaz de generalizar a nuevos ejemplos (no vistos).
En √°rboles esto se logra mediante el **prunning** o **early stopping (parada temprana)**.


]

---



## Ventajas ‚úîÔ∏è


* Los √°rboles son f√°ciles de interpretar aun cuando las relaciones entre predictores son complejas.

* Los modelos basados en un solo √°rbol (no es el caso de Random Forest y Boosting) se pueden representar gr√°ficamente aun cuando el n√∫mero de predictores es mayor de 3.

* Los √°rboles pueden, en teor√≠a, manejar tanto predictores num√©ricos como categ√≥ricos sin tener que crear variables dummy o one-hot-encoding. En la pr√°ctica, esto depende de la implementaci√≥n del algoritmo que tenga cada librer√≠a.

* Al tratarse de m√©todos no param√©tricos, no es necesario que se cumpla ning√∫n tipo de distribuci√≥n espec√≠fica.

* Por lo general, requieren mucha menos limpieza y preprocesado de los datos en comparaci√≥n con otros m√©todos de aprendizaje estad√≠stico (por ejemplo, no requieren estandarizaci√≥n).

---

## Ventajas ‚úîÔ∏è

* No se ven muy influenciados por outliers.

* Si para alguna observaci√≥n, el valor de un predictor no est√° disponible, a pesar de no poder llegar a ning√∫n nodo terminal, se puede conseguir una predicci√≥n empleando todas las observaciones que pertenecen al √∫ltimo nodo alcanzado. La precisi√≥n de la predicci√≥n se ver√° reducida pero al menos podr√° obtenerse.

* Son muy √∫tiles en la exploraci√≥n de datos, permiten identificar de forma r√°pida y eficiente las variables (predictores) m√°s importantes.

* Son capaces de seleccionar predictores de forma autom√°tica.

* Pueden aplicarse a problemas de regresi√≥n y clasificaci√≥n.


---

## Desventajas ‚ùå

* La capacidad predictiva de los modelos basados en un √∫nico √°rbol es bastante inferior a la conseguida con otros modelos. Esto es debido a su tendencia al overfitting y alta varianza. Sin embargo, existen t√©cnicas m√°s complejas que, haciendo uso de la combinaci√≥n de m√∫ltiples √°rboles (bagging, random forest, boosting), consiguen mejorar en gran medida este problema.

* Son sensibles a datos de entrenamiento desbalanceados (una de las clases domina sobre las dem√°s).

* Cuando tratan con predictores continuos, pierden parte de su informaci√≥n al categorizarlos en el momento de la divisi√≥n de los nodos.

* Tal y como se describe m√°s adelante, la creaci√≥n de las ramificaciones de los √°rboles se consigue mediante el algoritmo de recursive binary splitting. Este algoritmo identifica y eval√∫a las posibles divisiones de cada predictor acorde a una determinada medida (RSS, Gini, entrop√≠a‚Ä¶). Los predictores continuos tienen mayor probabilidad de contener, solo por azar, alg√∫n punto de corte √≥ptimo, por lo que suelen verse favorecidos en la creaci√≥n de los √°rboles.

* No son capaces de extrapolar fuera del rango de los predictores observado en los datos de entrenamiento.



---

## Paquetes de R 


* **tree y rpart**: permiten crear y representar √°rboles de regresi√≥n y clasificaci√≥n.

* **randomForest**: dispone de los principales algoritmos para crear modelos random forest. Destaca por su f√°cil uso, pero no por su rapidez.

* **ranger**: algoritmos para crear modelos random forest. Es similar a randomForest pero mucho m√°s r√°pido. Adem√°s, incorpora extremely randomized trees y quantile regression forests.

* **gbm**: dispone de los principales algoritmos de boosting. Este paquete ya no est√° mantenido, aunque es √∫til para explicar los conceptos, no se recomienda su uso en producci√≥n.

* **XGBoost**: esta librer√≠a permite acceder al algoritmo XGboost (Extra Gradient boosting). Una adaptaci√≥n de gradient boosting que destaca por su eficiencia y rapidez.




.footnote[*https://www.cienciadedatos.net/documentos/33_arboles_de_prediccion_bagging_random_forest_boosting*]

---
class: inverse, center, middle



# Manos en R üôå

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
