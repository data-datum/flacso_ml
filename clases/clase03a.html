<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>clase03a.utf8</title>
    <meta charset="utf-8" />
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/xaringanExtra_fit-screen/fit-screen.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


background-image: url(img/portada-flacso.png)
background-size: cover
class: animated slideInRight fadeOutLeft, middle



























# Machine Learning en Aplicaciones Espaciales


### Clase 3a. Ensembles. Boosting


---


## Boosting


.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[

### En boosting tenemos una creaci贸n secuencial de modelos, y se trata de reducir el sesgo del estimador combinando estos modelos. El foco se va a poner en los modelos que tienen una performance pobre. 

### 驴Por qu茅?

### Porque vamos a ponderar esos errores para no cometerlos a futuro. 

]



---

## Comparaci贸n con bagging


&lt;img src="img/bagging-vs-boosting.jpeg" width="80%" style="display: block; margin: auto;" /&gt;


.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[

Recordemos que en bagging vamos a construir modelos de manera independiente, y luego hacer un promedio de esas predicciones. La combinaci贸n de modelos suele ser mas potente que un algoritmo que el estimador base por separado. 


]

---


# Flavours de boosting


* **Adaboost**: es el primer algoritmo de Boosting y como algoritmo base utiliza "decision stumps", o 谩rboles de decisi贸n simples que constan de un nodo y dos ra铆ces. Solo opera mediante iteraciones y ponderaciones. 

* **Gradient Boosting**: es una generalizaci贸n de boosting para funciones de p茅rdida diferenciables. Es  un procedimiento preciso y efectivo que se puede usar para regresi贸n y clasificaci贸n. 


* **XGBoost**: a diferencia del anterior incluye tratamiento nativo de nulls, y adem谩s, soporta paralelizaci贸n. Esto significa que paraleliza la construcci贸n de cada 谩rbol, guardandolos en bloques de memoria pre-ordenados y los reutiliza. Se paraleliza el split-finding. 



---

class: inverse, center, middle

## AdaBoost

--

## =

--

## Adaptive Boosting

---

## Adaboost


* Adaboost = Adaptive Boosting





---

## Adaboost 

Boosting para clasificaci贸n
&lt;img src="img/boost1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

class: inverse, center, middle

--

## Veamos como sucede esto 

--

## Paso a paso


---


### Vamos a crear 谩rboles de decisi贸n a partir de estos datos


&lt;img src="img/ada1.png" width="80%" style="display: block; margin: auto;" /&gt;


---

#### Vamos a asignar un peso a cada muestra 


&lt;img src="img/ada2.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## 1er 谩rbol de decisi贸n

Primera variable: dolor de pecho

&lt;img src="img/ada3.png" width="100%" style="display: block; margin: auto;" /&gt;


---

## 1er 谩rbol de decisi贸n


Segunda rama del 谩rbol

&lt;img src="img/ada4.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## 2do 谩rbol de decisi贸n

Operamos con la segunda variable

&lt;img src="img/ada5.png" width="100%" style="display: block; margin: auto;" /&gt;



---

## 3er 谩rbol de decisi贸n

Operamos con la tercer variable
&lt;img src="img/ada6.png" width="100%" style="display: block; margin: auto;" /&gt;



---

## Calculemos el 铆ndice de Gini para nuestros 谩rboles

&lt;img src="img/ada7.png" width="100%" style="display: block; margin: auto;" /&gt;



---

## Veamos las importancias que debemos asignar


&lt;img src="img/ada8.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## Primer 谩rbol de decisi贸n


&lt;img src="img/ada9.png" width="100%" style="display: block; margin: auto;" /&gt;


---


## Segundo 谩rbol de decisi贸n
&lt;img src="img/ada10.png" width="100%" style="display: block; margin: auto;" /&gt;

---

class: inverse, center, middle


## Vamos a recalcular los weights


---

## Cuando hay un error

&lt;img src="img/ada11.png" width="100%" style="display: block; margin: auto;" /&gt;

---



## Cuando hay un acierto

&lt;img src="img/ada12.png" width="100%" style="display: block; margin: auto;" /&gt;

---


## Tenemos los nuevos weights



&lt;img src="img/ada13.png" width="100%" style="display: block; margin: auto;" /&gt;



---

## Normalizamos los weights


&lt;img src="img/ada14.png" width="100%" style="display: block; margin: auto;" /&gt;



---

class: inverse, center, middle


## 驴C贸mo van a influir

--


## estos nuevos weights?


---



&lt;img src="img/ada15.png" width="130%" style="display: block; margin: auto;" /&gt;









---

## Tasa de aprendizaje en Adaboost


&lt;img src="img/LR-boosting.png" width="100%" style="display: block; margin: auto;" /&gt;


---



## Algoritmo de Adaboost

&lt;img src="img/adaboost-algo.png" width="90%" style="display: block; margin: auto;" /&gt;


.footnote[Fuente: *Applied Predictive Modeling*]

---

class: inverse, center, middle

## Gradient Boosting Machines


---


## Gradient Boosting

* Al igual que AdaBoost es un algoritmo secuencial. 

* A diferencia de AdaBoost:
1. utilizamos 谩rboles de decisi贸n, pero no necesariamente, decision stumps. 

2. No computamos los pesos en cada iteraci贸n, sino que se optimiza una funci贸n de p茅rdida diferenciable (loss function), por ejemplo, MSE para regresi贸n. 




---

## Gradient Boosting (intuitivamente)


&lt;img src="img/gbm.png" width="70%" style="display: block; margin: auto;" /&gt;



---

## Gradient Boosting 




---

## Learning rate y n煤mero de 谩rboles


&lt;img src="img/efecto-learning-rate-gbm.png" width="90%" style="display: block; margin: auto;" /&gt;


.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[

**Cuanto m谩s peque帽o sea el learning rate del algoritmo, m谩s 谩rboles (n_estimators) vamos a necesitar para lograr resultados 贸ptimos**

]

.footnote[Fuente: *Hands On Machine Learning with Scikit-learn and Tensorflow*]

---


## Algoritmo Gradient Boosting 


&lt;img src="img/gradient-boosting-algo.png" width="100%" style="display: block; margin: auto;" /&gt;

.footnote[Fuente: *Applied Predictive Modeling*]

---

class: inverse, center, middle

## XGBoost

--

## =

--

## Extreme Gradient Boosting


---

## XGBoost

* Presentado originalmente por T.Chen &amp; C.Guestrin (2016). XGBoost: A Scalable Tree Boosting System

* Implementaci贸n escalable de GBM. 

* rboles de decisi贸n basados en CART. 

* Regularizaci贸n para penalizar la complejidad de los 谩rboles. 

* Derivadas de segundo orden para optimizar la funci贸n objetivo. 

* Opciones para hacer sampling de columnas y filas (similar a Random Forest). 




---

## XGBoost

* XGBoost = Extreme gradient boosting

--

**驴Qu茅 nos ofrece XGBoost a diferencia de GBM?**

--


&lt;img src="img/xgboost-adv.png" width="90%" style="display: block; margin: auto;" /&gt;


.footnote[*https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d*]

---

class: inverse, center, middle

## Optimizaciones que implementa XGBoost

## https://www.youtube.com/watch?v=oRrKeUCEbq8&amp;t=112s 


---

### De 谩rboles de decisi贸n a XGBoost

&lt;img src="img/boosting-hierarchy.jpeg" width="100%" style="display: block; margin: auto;" /&gt;

.footnote[Fuente: *https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d*]



---


## Tunning XGBoost en R


&lt;img src="img/xgboost-tunning.png" width="100%" style="display: block; margin: auto;" /&gt;




---

class: inverse, center, middle


# Manos en R 


---

## Bibliograf铆a


* Sebastian Raschka. 
https://www.youtube.com/watch?v=zblsrxc7XpM 

* Stochastic Gradient Boosting (paper) https://statweb.stanford.edu/~jhf/ftp/stobst.pdf 

* XGBoost: A Scalable Tree Boosting System (paper) https://arxiv.org/pdf/1603.02754.pdf 

* Videos de StatQuest

* AdaBoost: https://www.youtube.com/watch?v=LsK-xG1cLYA 

* Gradient Boosting (Parte 1): https://www.youtube.com/watch?v=3CC4N4z3GJc&amp;t=260s

* XGBoost (Parte 1): https://www.youtube.com/watch?v=OtD8wVaFm6E&amp;t=954s 

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
