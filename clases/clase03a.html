<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>clase03a.utf8</title>
    <meta charset="utf-8" />
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/xaringanExtra_fit-screen/fit-screen.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


background-image: url(img/portada-flacso.png)
background-size: cover
class: animated slideInRight fadeOutLeft, middle



























# Machine Learning en Aplicaciones Espaciales


### Clase 3a. Ensembles. Boosting


---


## Boosting


.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[

### En boosting tenemos una creaci칩n secuencial de modelos, y se trata de reducir el sesgo del estimador combinando estos modelos. El foco se va a poner en los modelos que tienen una performance pobre. 

### 쯇or qu칠?

### Porque vamos a ponderar esos errores para no cometerlos a futuro. 

]



---

## Comparaci칩n con bagging


&lt;img src="img/bagging-vs-boosting.jpeg" width="80%" style="display: block; margin: auto;" /&gt;


.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[

Recordemos que en bagging vamos a construir modelos de manera independiente, y luego hacer un promedio de esas predicciones. La combinaci칩n de modelos suele ser mas potente que un algoritmo que el estimador base por separado. 


]

---


# Flavours de boosting


* **Adaboost**: es el primer algoritmo de Boosting y como **algoritmo base utiliza "decision stumps"**, o 치rboles de decisi칩n simples que constan de un nodo y dos ra칤ces. Solo opera mediante iteraciones y ponderaciones. 

* **Gradient Boosting**: es una generalizaci칩n de boosting para funciones de p칠rdida diferenciables. Es  un procedimiento preciso y efectivo que se puede usar para regresi칩n y clasificaci칩n. 


* **XGBoost**: a diferencia del anterior incluye tratamiento nativo de nulls, y adem치s, soporta paralelizaci칩n. Esto significa que paraleliza la construcci칩n de cada 치rbol, guardandolos en bloques de memoria pre-ordenados y los reutiliza. Se paraleliza el split-finding. 



---

class: inverse, center, middle

## AdaBoost

--

## =

--

## Adaptive Boosting





---

## Adaboost 

**Boosting para clasificaci칩n**


&lt;img src="img/boost1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## Adaboost 

**Boosting para regresi칩n**



&lt;img src="img/boost-reg.png" width="90%" style="display: block; margin: auto;" /&gt;

---

class: inverse, center, middle

--

## Veamos como sucede esto 

--

## Paso a paso




---


### Vamos a crear 치rboles de decisi칩n a partir de estos datos


&lt;img src="img/ada1.png" width="80%" style="display: block; margin: auto;" /&gt;


---


#### Vamos a asignar un peso a cada muestra 

**Ese peso (weight) es 1/n, en este caso, 1/8**

&lt;img src="img/ada2.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## Decision stumps



.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[


Un **decision stump** es un 치rbol de decisi칩n de 1 nodo ra칤z y 2 nodos hijos. Es un 치rbol de decisi칩n muy simple. En el caso de tener una variable continua, en el nodo ra칤z tenemos un threshold, como en este caso. 



]



&lt;img src="img/decision-stump.png" width="60%" style="display: block; margin: auto;" /&gt;


.footnote[Fuente: https://en.wikipedia.org/wiki/Decision_stump]
---

## 1er 치rbol de decisi칩n

Primera variable: dolor de pecho

&lt;img src="img/ada3.png" width="100%" style="display: block; margin: auto;" /&gt;


---

## 1er 치rbol de decisi칩n


Segunda rama del 치rbol

&lt;img src="img/ada4.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## 2do 치rbol de decisi칩n

Operamos con la segunda variable

&lt;img src="img/ada5.png" width="100%" style="display: block; margin: auto;" /&gt;



---

## 3er 치rbol de decisi칩n

Operamos con la tercer variable
&lt;img src="img/ada6.png" width="100%" style="display: block; margin: auto;" /&gt;



---

class: inverse, center, middle

## Tenemos 3 치rboles de decisi칩n

--

## 쮺u치l va a ser el primer 치rbol a considerar?




---

## Calculemos el 칤ndice de Gini para nuestros 치rboles

&lt;img src="img/ada7.png" width="100%" style="display: block; margin: auto;" /&gt;



---

## Veamos las importancias que debemos asignar


&lt;img src="img/ada8.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## Primer 치rbol de decisi칩n


&lt;img src="img/ada9.png" width="100%" style="display: block; margin: auto;" /&gt;


---


## Segundo 치rbol de decisi칩n
&lt;img src="img/ada10.png" width="100%" style="display: block; margin: auto;" /&gt;

---

class: inverse, center, middle


## Vamos a recalcular los weights


---

## Cuando hay un error

&lt;img src="img/ada11.png" width="100%" style="display: block; margin: auto;" /&gt;

---



## Cuando hay un acierto

&lt;img src="img/ada12.png" width="100%" style="display: block; margin: auto;" /&gt;

---


## Tenemos los nuevos weights



&lt;img src="img/ada13.png" width="100%" style="display: block; margin: auto;" /&gt;



---

## Normalizamos los weights


&lt;img src="img/ada14.png" width="100%" style="display: block; margin: auto;" /&gt;



---

class: inverse, center, middle


## 쮺칩mo van a influir

--


## estos nuevos weights?


---



&lt;img src="img/ada15.png" width="130%" style="display: block; margin: auto;" /&gt;




---

class: inverse, center, middle


## Si tuviese un mayor weight,

--

## esa muestra va a repetirse varias veces

--

## en el nuevo dataset




---

class: inverse, center, middle, 

## Retomando...


---

## Adaboost (pasos)

1. Asignamos los pesos (1/n) a cada muestra.

2. Creamos decision stumps para cada variable del dataset. 

3. Calculammos el 칤ndice de Gini. 

4. Seg칰n el error se calcula el Amount of Say. 

5. Recalculamos los pesos. 

6. Se repite el proceso. 

7. Se suman todas las predicciones de todos los decision stumps. 


---

class: inverse, center, middle

## Finalmente...

---



## Algoritmo de Adaboost

&lt;img src="img/adaboost-algo.png" width="90%" style="display: block; margin: auto;" /&gt;


.footnote[Fuente: *Applied Predictive Modeling*]

---

class: inverse, center, middle

## Gradient Boosting Machines


---


## Gradient Boosting



.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[
* Al igual que AdaBoost es un algoritmo secuencial, y tiene en cuenta el error de los antiguos predictores. 

]




.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[

* A diferencia de AdaBoost:
1. utilizamos 치rboles de decisi칩n, pero no necesariamente, decision stumps. 

2. No computamos los pesos en cada iteraci칩n, sino que se optimiza una funci칩n de p칠rdida diferenciable (loss function), por ejemplo, MSE para regresi칩n. 

3. Para un ejemplo de regresi칩n, el nodo ra칤z, va a ser la media (average) de esa variable que intentamos predecir. 



]






---

class: inverse, center, middle

## Veamos paso a paso


---

## Vamos a predecir el *peso* a partir de estas variables


**Estamos ante un caso de regresi칩n**
&lt;img src="img/gbm1.png" width="90%" style="display: block; margin: auto;" /&gt;



---

## Calculamos la media de esa variable 

&lt;img src="img/gbm2.png" width="100%" style="display: block; margin: auto;" /&gt;

---

**A esa media le restamos el valor original de la variable**

Decimos pseudo-residuales porque los residuales son un concepto de regresi칩n lineal.

&lt;img src="img/gbm3.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## Primer 치rbol de decisi칩n

Uso los residuales en vez del valor verdadero 

&lt;img src="img/gbm4.png" width="100%" style="display: block; margin: auto;" /&gt;


---

**Los valores que estan en la misma rama, los promediamos**

&lt;img src="img/gbm5.png" width="100%" style="display: block; margin: auto;" /&gt;


---

**Si a la media aritm칠tica de todas las muestras, le sumo el valor del residual, vemos q coincide con el valor verdadero**

&lt;img src="img/gbm6.png" width="100%" style="display: block; margin: auto;" /&gt;


---


class: inverse, center, middle

## Vamos a introducir el concepto de 

--

## tasa de aprendizaje


---

## Tasa de aprendizaje

&lt;img src="img/learning-rate1.png" width="50%" style="display: block; margin: auto;" /&gt;


.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[

**Importante: definir una tasa de aprendizaje 칩ptima, ya que si ese valor es muy grande: el algoritmo no converge, pero si es muy peque침o: vamos a necesitar muchos pasos o 칠pocas para la convergencia**

]

---


**Agregamos la tasa de aprendizaje a nuestro modelo**

En este caso, definimos, una tasa de aprendizaje igual a 0.1. 


&lt;img src="img/gbm7.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## Recordemos

**En el contexto de una regresi칩n lineal, siempre intentamos disminuir los residuales ya que eso implica disminuir el error en la predicci칩n**


&lt;img src="img/reg-res.svg" width="100%" style="display: block; margin: auto;" /&gt;



---

## Finalmente...

**Los residuales han disminuido**
&lt;img src="img/gbm8.png" width="100%" style="display: block; margin: auto;" /&gt;


---

**Graficamente se ve as칤**


&lt;img src="img/gbm.png" width="70%" style="display: block; margin: auto;" /&gt;









---

## Learning rate y n칰mero de 치rboles


&lt;img src="img/efecto-learning-rate-gbm.png" width="90%" style="display: block; margin: auto;" /&gt;


.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[

**Cuanto m치s peque침o sea el learning rate del algoritmo, m치s 치rboles (n_estimators) vamos a necesitar para lograr resultados 칩ptimos**

]

.footnote[Fuente: *Hands On Machine Learning with Scikit-learn and Tensorflow*]

---


## Algoritmo Gradient Boosting 

*Est치 definido para clasificaci칩n*
&lt;img src="img/gradient-boosting-algo.png" width="100%" style="display: block; margin: auto;" /&gt;

.footnote[Fuente: *Applied Predictive Modeling*]

---

class: inverse, center, middle

## XGBoost

--

## =

--

## eXtreme Gradient Boosting


---

## XGBoost


.bg-near-white.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt2[

* Presentado originalmente por T.Chen &amp; C.Guestrin (2016). XGBoost: A Scalable Tree Boosting System

* Implementaci칩n escalable de GBM. 

* 츼rboles de decisi칩n basados en CART. 

* Regularizaci칩n para penalizar la complejidad de los 치rboles. 

* Derivadas de segundo orden para optimizar la funci칩n objetivo. 

* Opciones para hacer sampling de columnas y filas (similar a Random Forest). 


]

---

## XGBoost


--

**쯈u칠 nos ofrece XGBoost a diferencia de GBM?**

--


&lt;img src="img/xgboost-adv.png" width="90%" style="display: block; margin: auto;" /&gt;


.footnote[*https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d*]

---

class: center, middle

## Optimizaciones que implementa XGBoost

## https://www.youtube.com/watch?v=oRrKeUCEbq8&amp;t=112s 


---

### De 치rboles de decisi칩n a XGBoost

&lt;img src="img/boosting-hierarchy.jpeg" width="100%" style="display: block; margin: auto;" /&gt;

.footnote[Fuente: *https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d*]



---


## Tunning XGBoost en R


&lt;img src="img/xgboost-tunning.png" width="100%" style="display: block; margin: auto;" /&gt;




---

class: inverse, center, middle


# Manos en R 游뗿


---

## Bibliograf칤a


* Sebastian Raschka. 
https://www.youtube.com/watch?v=zblsrxc7XpM 

* Stochastic Gradient Boosting (paper) https://statweb.stanford.edu/~jhf/ftp/stobst.pdf 

* XGBoost: A Scalable Tree Boosting System (paper) https://arxiv.org/pdf/1603.02754.pdf 

* Videos de StatQuest

* AdaBoost: https://www.youtube.com/watch?v=LsK-xG1cLYA 

* Gradient Boosting (Parte 1): https://www.youtube.com/watch?v=3CC4N4z3GJc&amp;t=260s

* XGBoost (Parte 1): https://www.youtube.com/watch?v=OtD8wVaFm6E&amp;t=954s 

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
