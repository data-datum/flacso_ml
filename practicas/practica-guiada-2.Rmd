---
title: "Practica guiada 2 - Tunning de árboles"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Un primer modelo de Machine Learning

## Dataset

El dataset corresponde a Boston, que se encuentra disponible en Kaggle; una plataforma de competencia de machine learning. 
Más info en: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html 

#### Cargo las librerias

```{r eval=FALSE}
#si no tengo instalado la libreria, la instalo
#install.packages(tidymodels)
```


```{r warning=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(corrr)
library(MASS) #el dataset se encuentra en esta librería 
```

#### Ingreso los datos

```{r}
data(Boston)
```

### División de los datos

```{r}
p_split <- Boston %>%
  initial_split(prop=0.75) # divido en 75%
p_train <- training(p_split)
p_split
```


```{r}
p_folds <- vfold_cv(p_train, v=3)
```

```{r}
p_folds
```



### Preprocesamiento

```{r}
recipe_dt <- p_train %>%
  recipe(medv~.) %>%
  step_corr(all_predictors()) %>% #elimino las correlaciones
  step_center(all_predictors(), -all_outcomes()) %>% #centrado
  step_scale(all_predictors(), -all_outcomes()) %>% #escalado
  prep() 
```


### Modelo
 
Aca vamos a especificar las variables que vamos a hacer tunning, en este caso, con **random search**. 

```{r}
tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("regression")

tree_spec
```

Veamos los posibles valores 
```{r}
tree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 4)

tree_grid
```



```{r}
doParallel::registerDoParallel() #paralelizamos los cálculos

set.seed(345)
tree_rs <- tune_grid(
  tree_spec,
  medv ~ .,
  resamples = p_folds,
  grid = tree_grid,
  metrics = metric_set(rmse, rsq, mae, mape)
)

tree_rs
```


```{r}
collect_metrics(tree_rs)
```

```{r}
autoplot(tree_rs) 
```


```{r}
show_best(tree_rs)
```

```{r}
final_tree <- finalize_model(tree_spec, select_best(tree_rs, "rmse"))

final_tree
```

```{r}
final_fit <- fit(final_tree, medv ~ ., p_train)
final_rs <- last_fit(final_tree, medv ~ ., p_split)
```


```{r}

```


```{r}
final_rs %>%
  collect_metrics()
```


```{r}
collect_predictions(final_rs) %>%
  ggplot(aes(medv, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()
```


```{r}
library(vip)

final_fit %>%
  vip(geom = "col", aesthetics = list(fill = "midnightblue", alpha = 0.8)) +
  scale_y_continuous(expand = c(0, 0))
```





