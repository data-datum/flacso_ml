---
title: "Practica 1 - Introducción a ML y tidymodels"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Un primer modelo de Machine Learning

## Dataset

El dataset corresponde a Boston, que se encuentra disponible en Kaggle; una plataforma de competencia de machine learning. 
Más info en: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html 

#### Cargo las librerias

```{r eval=FALSE}
#si no tengo instalado la libreria, la instalo
#install.packages(tidymodels)
```


```{r warning=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(corrr)
library(MASS) #el dataset se encuentra en esta librería 
library(corrplot)
```

#### Ingreso los datos

```{r}
#ingreso el dataset que esta en la libreria 
data(Boston)
```

Con la función head() de rbase, nos permite ver las primeras filas del dataset
```{r}
head(Boston) #veamos el encabezado del dataset
```


Con la función glimpse() de dplyr podemos ver una breve descripción de las variables del dataset, ver que tipo de dato forman parte del dataset, etc. 
```{r}
glimpse(Boston)
```

Como podemos ver, todas las variables son cuantitativas continuas, lo que nos facilita no tener que codificarlas (en el caso que sean categóricas).
Las variables (columnas) en total son 14; de las cuales una es la variable que nos interesa predecir, en este caso, **medv**. Las otras variables van a ser las variables para nuestro modelo. 

### *Breve descripción de las variables*
(https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)

*crim*: tasa de criminalidad per cápita por ciudad.

*zn*: proporción de terreno residencial dividido en zonas para lotes de más de 25,000 pies cuadrados.

*indus*: proporción de acres comerciales no minoristas por ciudad.

*chas*: Variable ficticia de Charles River (= 1 si el tramo limita con el río; 0 en caso contrario).

*nox*: concentración de óxidos de nitrógeno (partes por 10 millones).

*rm*: número medio de habitaciones por vivienda.

*age*: Proporción de unidades ocupadas por sus propietarios construidas antes de 1940.

*dis*: media ponderada de las distancias a cinco centros de empleo de Boston.

*rad*:índice de accesibilidad a carreteras radiales.

*tax*: Tasa de impuesto a la propiedad de valor total por \ $ 10,000.

*ptratio*: Proporción alumno-profesor por ciudad.

*black*: 1000 (Bk - 0.63) ^ 2 donde Bk es la proporción de negros por ciudad.

*lstat*: estatus más bajo de la población (porcentaje).

*medv*: valor medio de las viviendas ocupadas por sus propietarios en \ $ 1000



## Pequeño análisis exploratorio de toda la base de datos 
Esta etapa nos ayuda a visualizar distribuciones de probabilidad de las variables, si hay algun outlier y demás. 



```{r}
cor(Boston)
```




### Vamos a dividir los datos

En esta instancia se deben dividir los datos en train y test. 


```{r}
set.seed(123)
p_split <- Boston %>%
  initial_split(prop=0.75) # divido en 75%
p_train <- training(p_split)
p_test <- testing(p_split)
p_split
```

Esta división es arbitraria, podemos hacer divisiones de 75/25, 80/20, 90/10, etc. 

```{r}
#Ejecutamos este comando para corrobar que la data de TRAIN haya sido randomizada al hacer el sampling. 
p_train %>%
  head()
```

Lo mismo para TEST, vemos si han sido tomados al azar los datos que forman parte de TEST. 
```{r}
p_test %>%
  head()
```


## Receta (Preprocesamiento de los datos)

Con este nombre agrupamos todos los **pre-procesamientos** que vamos a realizar al dataset de *train*, con lo cual haremos el modelado. Estas mismas operaciones se realizan al final en el dataset de *test*.


En este paso lo que vamos a hacer es eliminar las correlaciones, centrar, escalar los datos. 
```{r}
recipe_dt <- p_train %>%
  recipe(medv~.) %>%
  step_corr(all_predictors()) %>% #elimino las correlaciones
  step_center(all_predictors(), -all_outcomes()) %>% #centrado
  step_scale(all_predictors(), -all_outcomes()) %>% #escalado
  prep() 
```

Vemos la receta que hemos creado
```{r}
recipe_dt
```



## Modelo

En esta instancia vamos a elegir el modelo para el entrenamiento, tambien seleccionamos de que libreria vamos a tomar las funciones de R y también el tipo de tarea. 
En este caso vamos a hacer, un árbol de decisión, de la libreria rpart y una tarea de regresión. 
```{r}
set.seed(123)
tree_spec <- decision_tree() %>% #arboles de decisión
  set_engine("rpart") %>% #librería rpart
  set_mode("regression") #regresión
tree_spec
```

## Inicio el workflow

En workflow nos permite trabajar de manera ordenada en MACHINE LEARNING, y es una buena práctica utilizarlo siempre que tengamos más de un paso de pre-procesamiento. 

* Agregamos la receta mediante add_recipe(), y luego, 
* agregamos el modelo. 

Luego imprimo el workflow
```{r}
tree_wf <- workflow() %>%
  add_recipe(recipe_dt) %>% #agrego la receta
  add_model(tree_spec) #agrego el modelo
# Imprimo el workflow
tree_wf
```

### Error de estimacion del modelo en TRAIN

Esta es la primer etapa de nuestro modelo, vamos entrenarlo en esta etapa, antes SOLO LO DEFINIMOS NO HEMOS CALCULADO NADA. 
Con la función fit(), se realiza el entrenamiento en este caso. 

Depende de como decidamos modelar los datos, esta función fit(), puede cambiar por otra, por ejemplo, fit_resamples(), si hacemos validación cruzada. 
```{r}
set.seed(123) 

tree_fit <- tree_spec %>%
  fit(medv ~ ., data = p_train) 

#imprimo el modelo
tree_fit
```


## Error en TRAIN


En este paso lo que vamos a hacer es agregar la columna de la predicción a los datos de TRAIN, mediante la función bind_cols. 
Esto nos va a permitir ver la diferencia entre la predicción y el valor verdadero. 
```{r}
results <- p_train %>%
    bind_cols(predict(tree_fit, p_train) %>%
                  rename(.pred_tree = .pred))
```


Ahora vamos a ver las primeras filas de este nuevo dataset que hemos armado. 
```{r}
#veamos nuestra nueva tabla
head(results)
```


Vamos a calcular las metricas correspondientes del modelo. Para ello utilizamos la función metrics() disponible tambien en tidymodels. 
Agregamos los el dataset, y le decimos cual es el valor verdadero y cual es la predicción.
```{r}
metrics(results, truth = medv, estimate = .pred_tree)
```

### Error de estimacion del modelo en TEST

Esta etapa es la predicción del modelo de ML, y es el resultado que generalmente se reporta. 
IMPORTANTE: los resultados se deben reportar SI O SI, puede o no ir acompañado de los resultados de TRAIN. 

```{r}
results_test <- p_test %>%
    bind_cols(predict(tree_fit, p_test) %>%
                  rename(.pred_tree = .pred))
```

Vemos la primera fila del dataset de test con las predicciones al final. 
```{r}
head(results_test)
```

Ahora vemos las métricas. 
```{r}
metrics(results_test, truth = medv, estimate = .pred_tree)
```




Este es el modelo más simple que podemos obtener con tidymodels. 
La próxima clase este modelo va a complejizarse mucho mas. :)



## Vamos a plotear el árbol de decisión

Ingreso las librerias para hacer el plot del árbol. 
```{r message=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)
```

Plot del árbol, mediante dos funciones:
extract_fit() y rpart.plot(). 
```{r}
tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE, extra=1)
```



Ploteo las predicciones y el valor verdadero de los resultados en TEST. 
```{r}
results_test %>%
    ggplot(aes(medv, .pred_tree)) +
    geom_abline(lty = 2, color = "gray50") +
    geom_point(size = 1.5, alpha = 0.3, show.legend = FALSE)
```








