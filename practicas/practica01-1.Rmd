---
title: "Practica 1 - Introducción a ML y tidymodels"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Un primer modelo de Machine Learning

## Dataset

El dataset corresponde a Boston, que se encuentra disponible en Kaggle; una plataforma de competencia de machine learning. 
Más info en: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html 

#### Cargo las librerias

```{r eval=FALSE}
#si no tengo instalado la libreria, la instalo
#install.packages(tidymodels)
```


```{r warning=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(corrr)
library(MASS) #el dataset se encuentra en esta librería 
```

#### Ingreso los datos

```{r}
data(Boston)
```

```{r}
head(Boston) #veamos el encabezado del dataset
```



```{r}
glimpse(Boston)
```

Como podemos ver, todas las variables son cuantitativas continuas, lo que nos facilita no tener que codificarlas (en el caso que sean categóricas).
Las variables (columnas) en total son 14; de las cuales una es la variable que nos interesa predecir, en este caso, **medv**. Las otras variables van a ser las variables para nuestro modelo. 

### *Breve descripción de las variables*
(https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)

*crim*: tasa de criminalidad per cápita por ciudad.

*zn*: proporción de terreno residencial dividido en zonas para lotes de más de 25,000 pies cuadrados.

*indus*: proporción de acres comerciales no minoristas por ciudad.

*chas*: Variable ficticia de Charles River (= 1 si el tramo limita con el río; 0 en caso contrario).

*nox*: concentración de óxidos de nitrógeno (partes por 10 millones).

*rm*: número medio de habitaciones por vivienda.

*age*: Proporción de unidades ocupadas por sus propietarios construidas antes de 1940.

*dis*: media ponderada de las distancias a cinco centros de empleo de Boston.

*rad*:índice de accesibilidad a carreteras radiales.

*tax*: Tasa de impuesto a la propiedad de valor total por \ $ 10,000.

*ptratio*: Proporción alumno-profesor por ciudad.

*black*: 1000 (Bk - 0.63) ^ 2 donde Bk es la proporción de negros por ciudad.

*lstat*: estatus más bajo de la población (porcentaje).

*medv*: valor medio de las viviendas ocupadas por sus propietarios en \ $ 1000



## Pequeño análisis exploratorio de toda la base de datos 
Esta etapa nos ayuda a visualizar distribuciones de probabilidad de las variables, si hay algun outlier y demás. 

```{r}
cor(Boston)
```


### Vamos a dividir los datos

En esta instancia se deben dividir los datos en train y test. 


```{r}
set.seed(123)
p_split <- Boston %>%
  initial_split(prop=0.75) # divido en 75%
p_train <- training(p_split)
p_test <- testing(p_split)
p_split
```

Esta división es arbitraria, podemos hacer divisiones de 75/25, 80/20, 90/10, etc. 

```{r}
#Ejecutamos este comando para corrobar que la data de TRAIN haya sido randomizada al hacer el sampling. 
p_train %>%
  head()
```


```{r}
p_test %>%
  head()
```


## Receta (Preprocesamiento de los datos)

Con este nombre agrupamos todos los **pre-procesamientos** que vamos a realizar al dataset de *train*, con lo cual haremos el modelado. Estas mismas operaciones se realizan al final en el dataset de *test*.

```{r}
recipe_dt <- p_train %>%
  recipe(medv~.) %>%
  step_corr(all_predictors()) %>% #elimino las correlaciones
  step_center(all_predictors(), -all_outcomes()) %>% #centrado
  step_scale(all_predictors(), -all_outcomes()) %>% #escalado
  prep() 
```

```{r}
recipe_dt
```



## Modelo
```{r}
set.seed(123)
tree_spec <- decision_tree() %>% #arboles de decisión
  set_engine("rpart") %>% #librería rpart
  set_mode("regression") #regresión
tree_spec
```


```{r}
tree_wf <- workflow() %>%
  add_recipe(recipe_dt) %>% #agrego la receta
  add_model(tree_spec) #agrego el modelo
tree_wf
```

### Error de estimacion del modelo en TRAIN
```{r}
set.seed(123) 
tree_fit <- tree_spec %>%
  fit(medv ~ ., data = p_train) 

tree_fit
```



```{r}
results <- p_train %>%
    bind_cols(predict(tree_fit, p_train) %>%
                  rename(.pred_tree = .pred))
```


```{r}
#veamos nuestra nueva tabla
head(results)
```

```{r}
metrics(results, truth = medv, estimate = .pred_tree)
```

### Error de estimacion del modelo en TEST

```{r}
results_test <- p_test %>%
    bind_cols(predict(tree_fit, p_test) %>%
                  rename(.pred_tree = .pred))
```


```{r}
head(results_test)
```


```{r}
metrics(results_test, truth = medv, estimate = .pred_tree)
```




Este es el modelo más simple que podemos obtener con tidymodels. 
La próxima clase este modelo va a complejizarse mucho mas. :)



## Vamos a plotear el árbol de decisión


```{r message=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)
```


```{r}
tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE, extra=1)
```


```{r}
results %>%
    ggplot(aes(medv, .pred_tree)) +
    geom_abline(lty = 2, color = "gray50") +
    geom_point(size = 1.5, alpha = 0.3, show.legend = FALSE)
```








