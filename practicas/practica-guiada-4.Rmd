---
title: "Practica guiada 4 - Random Forest Regressor con Tunning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Regresión con Random Forest con Tunning

## Dataset

El dataset corresponde a Boston, que se encuentra disponible en Kaggle; una plataforma de competencia de machine learning. 
Más info en: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html 


#### Cargo las librerias

```{r warning=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(corrr)
library(MASS) #el dataset se encuentra en esta librería 
```

#### Ingreso los datos

```{r}
data(Boston)
```



### División de los datos

Voy a dividir los datos en train y test
```{r}
set.seed(1234)
p_split <- Boston %>%
    initial_split(prop = 0.75)
p_train <- training(p_split)
p_test <- testing(p_split)

glimpse(p_train)
```



Los datos de TRAIN voy a dividirlos en 3-folds para hacer validación cruzada v-folds=3
```{r}
p_folds <- vfold_cv(p_train, v=3, repeats = 5)
```

```{r}
p_folds
```


Veamos los splits de validación cruzada, son 3 folds que se repiten 5 veces. 
```{r}
p_folds$splits
```



## Datos de TEST
```{r}
head(p_test)
```



### Preprocesamiento de los datos

```{r}
recipe_rf <- p_train %>%
  recipe(medv~.) %>%
  step_corr(all_predictors()) %>% #elimino las correlaciones
  step_center(all_predictors(), -all_outcomes()) %>% #centrado
  step_scale(all_predictors(), -all_outcomes()) %>% #escalado
  prep() 
```


## Modelo de Random Forest

```{r}
rf_tune <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>%
  set_mode("regression") %>%
  set_engine("ranger")
```


## Workflow


```{r}
tune_wf <- workflow() %>%
  add_recipe(recipe_rf) %>%
  add_model(rf_tune)
```




## Entrenamiento del Modelo

```{r}
set.seed(8577)
doParallel::registerDoParallel()
ranger_tune <-tune_grid(tune_wf,
    resamples = p_folds,
    grid = 11
  )
```


```{r}
show_best(ranger_tune, metric = "rmse")
```




```{r}
ranger_tune %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  dplyr::select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "rmse")

```



```{r}
show_best(ranger_tune, "rmse")

```


```{r}
best_rmse <- select_best(ranger_tune, "rmse")
best_rmse
```

```{r}
final_rf <- finalize_workflow(
  tune_wf,
  best_rmse
)

final_rf
```


## Predicción en TEST
```{r}
final_res <- last_fit(final_rf, p_split)

collect_metrics(final_res)
```


```{r}
collect_predictions(final_res) %>%
  ggplot(aes(medv, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()
```

## Importancia de variables

```{r message=FALSE, warning=FALSE}
library(vip)

final_random_forest <- finalize_model(rf_tune, best_rmse)

final_random_forest %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(medv ~ .,
      data = p_train) %>%
  vip::vip(geom = "point")
```
