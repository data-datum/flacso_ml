---
title: "Practica guiada 3 - Random Forest Regressor"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Regresión con Random Forest (Baseline)

## Dataset

El dataset corresponde a Boston, que se encuentra disponible en Kaggle; una plataforma de competencia de machine learning. 
Más info en: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html 



#### Cargo las librerias

```{r warning=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(corrr)
library(MASS) #el dataset se encuentra en esta librería 
```

#### Ingreso los datos

```{r}
data(Boston)
```

### División de los datos

Voy a dividir los datos en train y test
```{r}
set.seed(1234)
p_split <- Boston %>%
    initial_split(prop = 0.75)
p_train <- training(p_split)
p_test <- testing(p_split)

glimpse(p_train)
```


Los datos de TRAIN voy a dividirlos en 3-folds para hacer validación cruzada v-folds=3
```{r}
p_folds <- vfold_cv(p_train, v=3, repeats = 5)
```

```{r}
p_folds
```

```{r}
head(p_test)
```



### Preprocesamiento de los datos

```{r}
recipe_rf <- p_train %>%
  recipe(medv~.) %>%
  step_corr(all_predictors()) %>% #elimino las correlaciones
  step_center(all_predictors(), -all_outcomes()) %>% #centrado
  step_scale(all_predictors(), -all_outcomes()) %>% #escalado
  prep() 
```


### Especificamos el modelo de ML
 
En esta etapa se especifica el modelo que vamos a implementar, en este caso, **Random Forest Baseline**. 

```{r}
rf_spec <- rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("regression")
```

### Workflow

Inicializo el workflow para trabajar de manera ordenada, luego visualizo los pasos. 
```{r}
rf_wf <- workflow() %>%
    add_recipe(recipe_rf) %>%
    add_model(rf_spec)

rf_wf
```



### Entrenamiento del Modelo
```{r}
set.seed(123)
rf_res <- rf_wf %>%
    fit_resamples(
        p_folds,
        control = control_resamples(save_pred = TRUE)
    )

glimpse(rf_res)
```



## Estos son los resultados de TRAIN 
```{r}
rf_res %>%
  collect_metrics()
```

## Finalizamos el workflow

```{r}
final_model <- finalize_model(rf_spec, select_best(rf_res, "rmse"))

final_model
```

## Predicción en TEST set


```{r}
final_rs <- last_fit(final_model, medv ~ ., p_split)
final_rs
```


```{r}
final_rs %>%
  collect_metrics()
```




```{r}
final_rs %>% 
    collect_predictions() 
```



```{r}
collect_predictions(final_rs) %>%
  ggplot(aes(medv, .pred)) +
  geom_abline(lty = 2, color = "gray50") +
  geom_point(alpha = 0.5, color = "midnightblue") +
  coord_fixed()
```


### Importancia de las variables

Vamos a plotear las variables más importantes del MODELO, mediante la librería vip.
Más información sobre esta librería en https://koalaverse.github.io/vip/ 


```{r}
library(vip)

final_model %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(medv ~ .,
      data = juice(recipe_rf)) %>%
  vip(geom = "point")
```

