---
title: "Practica 3 Individual - Random Forest / XGBoost"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
```


## Dataset


El dataset se encuentra disponible en: 

https://www.kaggle.com/camnugent/california-housing-prices

Lo podemos ingresar mediante el siguiente comando

```{r}
train <- read_csv("https://raw.githubusercontent.com/data-datum/datasets/main/housing.csv")
```


**La variable a predecir es la de median_house_value.**


## Consignas

1. Implemente un modelo de Random Forest, puede ser con o sin Tunning de Hiperparámetros, siguiendo alguna de las prácticas guiadas. 

2. Implemente un modelo de XGBoost con Tunning siguiendo la práctica guiada correspondiente. 

3. ¿Con qué modelo obtuvo menor error? ¿Es conveniente un modelo más complejo? Discuta. 

